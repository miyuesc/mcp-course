# MCP 能力

MCP 服务器通过通信协议向客户端公开各种能力。这些能力分为四个主要类别，每个类别都有不同的特征和用例。让我们探索这些构成 MCP 功能基础的核心原语。

<Tip>

在本节中，我们将以每种语言中与框架无关的函数形式展示示例。这是为了专注于概念以及它们如何协同工作，而不是任何框架的复杂性。

在接下来的单元中，我们将展示这些概念如何在 MCP 特定代码中实现。

</Tip>

## 工具（Tools）

工具是 AI 模型可以通过 MCP 协议调用的可执行函数或操作。

- **控制**：工具通常是**模型控制的**，这意味着 AI 模型（LLM）根据用户的请求和上下文决定何时调用它们。
- **安全性**：由于工具能够执行具有副作用的操作，工具执行可能是危险的。因此，它们通常需要明确的用户批准。
- **用例**：发送消息、创建工单、查询 API、执行计算。

**示例**：一个获取指定位置当前天气数据的天气工具：

<hfoptions id="tool-example">
<hfoption id="python">

```python
def get_weather(location: str) -> dict:
    """获取指定位置的当前天气。"""
    # 连接到天气 API 并获取数据
    return {
        "temperature": 72,
        "conditions": "Sunny",
        "humidity": 45
    }
```

</hfoption>
<hfoption id="javascript">

```javascript
function getWeather(location) {
    // 连接到天气 API 并获取数据
    return {
        temperature: 72,
        conditions: 'Sunny',
        humidity: 45
    };
}
```

</hfoption>
</hfoptions>

## 资源（Resources）

资源提供对数据源的只读访问，允许 AI 模型在不执行复杂逻辑的情况下检索上下文。

- **控制**：资源是**应用程序控制的**，这意味着主机应用程序通常决定何时访问它们。
- **性质**：它们设计用于数据检索，计算量最小，类似于 REST API 中的 GET 端点。
- **安全性**：由于它们是只读的，通常比工具呈现更低的安全风险。
- **用例**：访问文件内容、检索数据库记录、读取配置信息。

**示例**：一个提供文件内容访问的资源：

<hfoptions id="resource-example">
<hfoption id="python">

```python
def read_file(file_path: str) -> str:
    """读取指定路径文件的内容。"""
    with open(file_path, 'r') as f:
        return f.read()
```

</hfoption>
<hfoption id="javascript">

```javascript
function readFile(filePath) {
    // 使用 fs.readFile 读取文件内容
    const fs = require('fs');
    return new Promise((resolve, reject) => {
        fs.readFile(filePath, 'utf8', (err, data) => {
            if (err) {
                reject(err);
                return;
            }
            resolve(data);
        });
    });
}
```

</hfoption>
</hfoptions>

## 提示（Prompts）

提示是预定义的模板或工作流程，用于指导用户、AI 模型和服务器能力之间的交互。

- **目的**：它们构建交互以优化使用可用的工具和资源。
- **选择**：用户通常在 AI 模型开始处理之前选择提示，为交互设置上下文。
- **用例**：常见工作流程、专门的任务模板、引导式交互。

**示例**：一个用于生成代码审查的提示模板：

<hfoptions id="prompt-example">
<hfoption id="python">

```python
def code_review(code: str, language: str) -> list:
    """为提供的代码片段生成代码审查。"""
    return [
        {
            "role": "system",
            "content": f"你是一个代码审查员，正在检查 {language} 代码。提供详细的审查，突出最佳实践、潜在问题和改进建议。"
        },
        {
            "role": "user",
            "content": f"请审查这个 {language} 代码：\n\n```{language}\n{code}\n```"
        }
    ]
```

</hfoption>
<hfoption id="javascript">

```javascript
function codeReview(code, language) {
    return [
        {
            role: 'system',
            content: `你是一个代码审查员，正在检查 ${language} 代码。提供详细的审查，突出最佳实践、潜在问题和改进建议。`
        },
        {
            role: 'user',
            content: `请审查这个 ${language} 代码：\n\n\`\`\`${language}\n${code}\n\`\`\``
        }
    ];
}
```

</hfoption>
</hfoptions>

## 采样（Sampling）

采样允许服务器请求客户端（特别是主机应用程序）执行 LLM 交互。

- **控制**：采样是**服务器发起的**，但需要客户端/主机的协助。
- **目的**：它启用服务器驱动的代理行为和潜在的递归或多步骤交互。
- **安全性**：与工具一样，采样操作通常需要用户批准。
- **用例**：复杂的多步骤任务、自主代理工作流程、交互式过程。

**示例**：服务器可能请求客户端分析它已处理的数据：

<hfoptions id="sampling-example">
<hfoption id="python">

```python
def request_sampling(messages, system_prompt=None, include_context="none"):
    """从客户端请求 LLM 采样。"""
    # 在实际实现中，这将向客户端发送请求
    return {
        "role": "assistant",
        "content": "对提供数据的分析..."
    }
```

</hfoption>
<hfoption id="javascript">

```javascript
function requestSampling(messages, systemPrompt = null, includeContext = 'none') {
    // 在实际实现中，这将向客户端发送请求
    return {
        role: 'assistant',
        content: '对提供数据的分析...'
    };
}

function handleSamplingRequest(request) {
    const { messages, systemPrompt, includeContext } = request;
    // 在实际实现中，这将处理请求并返回响应
    return {
        role: 'assistant',
        content: '对采样请求的响应...'
    };
}
```

</hfoption>
</hfoptions>

采样流程遵循以下步骤：
1. 服务器向客户端发送 `sampling/createMessage` 请求
2. 客户端审查请求并可以修改它
3. 客户端从 LLM 采样
4. 客户端审查完成结果
5. 客户端将结果返回给服务器

<Tip>

这种人在环路的设计确保用户保持对 LLM 看到和生成内容的控制。在实现采样时，重要的是提供清晰、结构良好的提示并包含相关上下文。

</Tip>

## 能力如何协同工作

让我们看看这些能力如何协同工作以实现复杂的交互。在下表中，我们概述了能力、控制者、控制方向和其他一些细节。

| 能力 | 控制者 | 方向 | 副作用 | 需要批准 | 典型用例 |
|------|--------|------|--------|----------|----------|
| 工具 | 模型（LLM） | 客户端 → 服务器 | 是（潜在的） | 是 | 操作、API 调用、数据操作 |
| 资源 | 应用程序 | 客户端 → 服务器 | 否（只读） | 通常否 | 数据检索、上下文收集 |
| 提示 | 用户 | 服务器 → 客户端 | 否 | 否（用户选择） | 引导式工作流程、专门模板 |
| 采样 | 服务器 | 服务器 → 客户端 → 服务器 | 间接地 | 是 | 多步骤任务、代理行为 |

这些能力设计为以互补的方式协同工作：

1. 用户可能选择一个**提示**来开始专门的工作流程
2. 提示可能包含来自**资源**的上下文
3. 在处理过程中，AI 模型可能调用**工具**来执行特定操作
4. 对于复杂操作，服务器可能使用**采样**来请求额外的 LLM 处理

这些原语之间的区别为 MCP 交互提供了清晰的结构，使 AI 模型能够访问信息、执行操作并参与复杂的工作流程，同时保持适当的控制边界。

## 发现过程

MCP 的关键特性之一是动态能力发现。当客户端连接到服务器时，它可以通过特定的列表方法查询可用的工具、资源和提示：

- `tools/list`：发现可用的工具
- `resources/list`：发现可用的资源
- `prompts/list`：发现可用的提示

这种动态发现机制允许客户端适应每个服务器提供的特定能力，而无需硬编码服务器功能的知识。

## 结论

理解这些核心原语对于有效使用 MCP 至关重要。通过提供具有明确控制边界的不同类型能力，MCP 在保持适当的安全和控制机制的同时，实现了 AI 模型与外部系统之间的强大交互。

在下一节中，我们将探索 Gradio 如何与 MCP 集成，为这些能力提供易于使用的界面。